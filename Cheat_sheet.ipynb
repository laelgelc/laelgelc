{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810d9f10-ec19-4b09-8f90-e983e460b319",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c2c96-2fc3-4a1a-995b-c388036a2a15",
   "metadata": {},
   "source": [
    "# Cheat sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c2af7-9fc1-4f51-a4f5-2ed915b93039",
   "metadata": {},
   "source": [
    "## Archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fab8c-10c9-4085-ac0a-2e5ff4c2df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a compressed archive file of the contents of the `cl_images` directory\n",
    "tar czvf cl_images_eyamrog.tar.gz cl_images\n",
    "\n",
    "# Creates a compressed archive file. The `-C` option changes the directory to `cl_images` before adding its contents to the archive file\n",
    "tar czvf cl_images_eyamrog.tar.gz -C cl_images .\n",
    "\n",
    "# Extracts the contents of the archive file\n",
    "tar xzvf cl_images_eyamrog.tar.gz\n",
    "\n",
    "# Extracts the contents of the archive file. The `-C` option is used to change the directory to `cl_images` before extracting the files from the archive \n",
    "mkdir cl_images\n",
    "tar xzvf cl_images_eyamrog.tar.gz -C cl_images\n",
    "\n",
    "# Installs `zip` and Ã¹nzip` commands\n",
    "sudo apt install -y zip\n",
    "\n",
    "# Creates a compressed `.zip` file of the contents of the `directory` directory\n",
    "zip -r directory.zip /path/to/directory\n",
    "\n",
    "# Extracts the contents of the `.zip` archive into the `directory` directory\n",
    "mkdir /path/to/directory\n",
    "unzip file.zip -d /path/to/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f28a2a-bdcb-44b4-ac86-0a6c95097097",
   "metadata": {},
   "source": [
    "## Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad044d8-3feb-465b-9210-b73481e3573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/laelgelc/cl_images.git\n",
    "\n",
    "git config --global --edit\n",
    "\n",
    "git status\n",
    "\n",
    "git add <file>\n",
    "\n",
    "git add .\n",
    "\n",
    "git stash\n",
    "\n",
    "git pull\n",
    "\n",
    "git commit -m \"20231221-1156\"\n",
    "\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25978d0c-3455-4db8-875f-4b6dfbb0717e",
   "metadata": {},
   "source": [
    "## Amazon S3 Bucket policy for bucket public access\n",
    "Replace 'laelgelcclimages' by the corresponding bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085165da-0f38-437d-bc59-755f82595df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": \"*\",\n",
    "            \"Action\": \"s3:GetObject\",\n",
    "            \"Resource\": \"arn:aws:s3:::laelgelcclimages/*\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf6db5-d36f-42dd-a94d-c72591c441ff",
   "metadata": {},
   "source": [
    "## Google Cloud [Application Default Credentials](https://cloud.google.com/docs/authentication/provide-credentials-adc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228999c-6dbf-4571-b586-71f40aa8568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubuntu@ip-172-31-19-178:~/my_env$ source bin/activate\n",
    "(my_env) ubuntu@ip-172-31-19-178:~/my_env$ gcloud auth application-default login\n",
    "Go to the following link in your browser:\n",
    "\n",
    "    https://<omitted>\n",
    "\n",
    "Enter authorization code: <omitted>\n",
    "\n",
    "Credentials saved to file: [/home/ubuntu/.config/gcloud/application_default_credentials.json]\n",
    "\n",
    "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
    "WARNING: \n",
    "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n",
    "(my_env) ubuntu@ip-172-31-19-178:~/my_env$ gcloud auth application-default set-quota-project <omitted>\n",
    "\n",
    "Credentials saved to file: [/home/ubuntu/.config/gcloud/application_default_credentials.json]\n",
    "\n",
    "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
    "\n",
    "Quota project \"<omitted>\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n",
    "(my_env) ubuntu@ip-172-31-19-178:~/my_env$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b256e3a0",
   "metadata": {},
   "source": [
    "## OpenSSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329037ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "eyamrog@RogLet-ASUS:~$ sudo apt install putty-tools\n",
    "eyamrog@RogLet-ASUS:~$ cp /mnt/c/Users/eyamr/OneDrive/Documentos/0-Technology/LAELGELC20231117.ppk .\n",
    "eyamrog@RogLet-ASUS:~$ puttygen LAELGELC20231117.ppk -O private-openssh -o LAELGELC20231117.pem\n",
    "eyamrog@RogLet-ASUS:~$ chmod 400 LAELGELC20231117.pem\n",
    "eyamrog@RogLet-ASUS:~$ eval \"$(ssh-agent -s)\"\n",
    "Agent pid 924\n",
    "eyamrog@RogLet-ASUS:~$ ssh-add LAELGELC20231117.pem\n",
    "Identity added: LAELGELC20231117.pem (LAELGELC20231117.pem)\n",
    "eyamrog@RogLet-ASUS:~$ ssh -A -t ubuntu@ec2-18-231-160-175.sa-east-1.compute.amazonaws.com\n",
    "The authenticity of host 'ec2-18-231-160-175.sa-east-1.compute.amazonaws.com (18.231.160.175)' can't be established.\n",
    "ED25519 key fingerprint is SHA256:a/MtF4Zlyzxv9OIljp3Jr/BY2emQcl/6ZFHETKkmrjk.\n",
    "This key is not known by any other names\n",
    "Are you sure you want to continue connecting (yes/no/[fingerprint])? yes\n",
    "Warning: Permanently added 'ec2-18-231-160-175.sa-east-1.compute.amazonaws.com' (ED25519) to the list of known hosts.\n",
    "Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 6.2.0-1016-aws x86_64)\n",
    "\n",
    " * Documentation:  https://help.ubuntu.com\n",
    " * Management:     https://landscape.canonical.com\n",
    " * Support:        https://ubuntu.com/advantage\n",
    "\n",
    "  System information as of Sat Dec  2 15:29:51 UTC 2023\n",
    "\n",
    "  System load:  0.080078125       Processes:             96\n",
    "  Usage of /:   22.0% of 9.51GB   Users logged in:       0\n",
    "  Memory usage: 20%               IPv4 address for eth0: 172.31.36.94\n",
    "  Swap usage:   0%\n",
    "\n",
    "\n",
    "Expanded Security Maintenance for Applications is not enabled.\n",
    "\n",
    "0 updates can be applied immediately.\n",
    "\n",
    "Enable ESM Apps to receive additional future security updates.\n",
    "See https://ubuntu.com/esm or run: sudo pro status\n",
    "\n",
    "\n",
    "Last login: Sat Dec  2 15:14:29 2023 from 189.120.73.98\n",
    "ubuntu@ip-172-31-36-94:~$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c524f4e",
   "metadata": {},
   "source": [
    "## AWS CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00707a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Listing the objects in a bucket with corresponding sizes and redirecting the output into a file\n",
    "aws s3 ls s3://laelgelctweets/ --human-readable --summarize > files.txt\n",
    "\n",
    "# Creating folders in a bucket\n",
    "PS C:\\Users\\eyamr> aws s3api put-object --bucket gelctweets --key 2019_01/\n",
    "{\n",
    "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n",
    "    \"ServerSideEncryption\": \"AES256\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc70b0",
   "metadata": {},
   "source": [
    "## Anaconda Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d932a-c4d7-43f2-a879-c9c760aaa7bc",
   "metadata": {},
   "source": [
    "Python repositories:\n",
    "- https://anaconda.org/\n",
    "- https://pypi.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed998f17-9a7a-4834-b9d6-bc8f237c9129",
   "metadata": {},
   "source": [
    "### Environment management - via `condaenv.xml` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49ccf1-a3aa-4e0f-8e39-9349d162831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(base) C:\\Users\\eyamr>cd Downloads\n",
    "\n",
    "(base) C:\\Users\\eyamr\\Downloads>dir\n",
    " Volume in drive C is OS\n",
    " Volume Serial Number is B268-C40D\n",
    "\n",
    " Directory of C:\\Users\\eyamr\\Downloads\n",
    "\n",
    "02/08/2024  10:59    <DIR>          .\n",
    "19/07/2024  07:37    <DIR>          ..\n",
    "02/08/2024  11:08               412 condaenv.yml\n",
    "02/08/2024  11:04               809 scratchpad.sh\n",
    "               2 File(s)          1.221 bytes\n",
    "               2 Dir(s)  898.125.189.120 bytes free\n",
    "\n",
    "(base) C:\\Users\\eyamr\\Downloads>conda env create -f condaenv.yml\n",
    "<omitted>\n",
    "\n",
    "\n",
    "(base) C:\\Users\\eyamr\\Downloads>conda env list\n",
    "# conda environments:\n",
    "#\n",
    "base                  *  C:\\Users\\eyamr\\anaconda3\n",
    "my_env                   C:\\Users\\eyamr\\anaconda3\\envs\\my_env\n",
    "\n",
    "\n",
    "(base) C:\\Users\\eyamr\\Downloads>conda activate my_env\n",
    "\n",
    "(my_env) C:\\Users\\eyamr\\Downloads>conda list\n",
    "<omitted>\n",
    "\n",
    "\n",
    "(my_env) C:\\Users\\eyamr\\Downloads>pip install gogettr truthbrush webvtt-py\n",
    "<omitted>\n",
    "\n",
    "\n",
    "(my_env) C:\\Users\\eyamr\\Downloads>conda deactivate\n",
    "\n",
    "(base) C:\\Users\\eyamr\\Downloads>exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217b3df-871f-4915-9e91-c20a3600ec23",
   "metadata": {},
   "source": [
    "### Environment management - manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca8b23-550f-404a-99d3-ce2cbd7ba17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(base) C:\\Users\\eyamr>conda env list\n",
    "# conda environments:\n",
    "#\n",
    "base                  *  C:\\Users\\eyamr\\anaconda3\n",
    "Env20240401              C:\\Users\\eyamr\\anaconda3\\envs\\Env20240401\n",
    "\n",
    "\n",
    "(base) C:\\Users\\eyamr>conda remove --name Env20240401 --all\n",
    "(base) C:\\Users\\eyamr>conda create --name my_env\n",
    "(base) C:\\Users\\eyamr>conda activate my_env\n",
    "(my_env) C:\\Users\\eyamr>conda list\n",
    "# packages in environment at C:\\Users\\eyamr\\anaconda3\\envs\\my_env:\n",
    "#\n",
    "# Name                    Version                   Build  Channel\n",
    "\n",
    "(my_env) C:\\Users\\eyamr> conda install beautifulsoup4\n",
    "(my_env) C:\\Users\\eyamr>conda install gogettr\n",
    "<omitted>\n",
    "PackagesNotFoundError: The following packages are not available from curren:\n",
    "<omitted>\n",
    "(my_env) C:\\Users\\eyamr>pip install gogettr\n",
    "(my_env) C:\\Users\\eyamr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4bffc4-629e-48c0-afb4-37ab30f4586f",
   "metadata": {},
   "source": [
    "### Cloud Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83691d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "(base) 22:41 ~/LAEL GELC $ conda create --name env20231215 python=3.10 ipykernel -y\n",
    "\n",
    "(base) 22:47 ~/LAEL GELC $ conda activate env20231215                                                                                                                           \n",
    "(env20231215) 22:47 ~/LAEL GELC $ conda list   \n",
    "\n",
    "(env20231215) 22:57 ~/LAEL GELC $ conda install -c conda-forge pysmartdl\n",
    "\n",
    "(env20231215) 22:50 ~/LAEL GELC $ conda install pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5d135-b393-4373-9815-f3c927e58c03",
   "metadata": {},
   "source": [
    "## Notes about Research Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d78ed7-bd3e-4268-8539-3bcd3b007bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ ll\n",
    "total 1224644\n",
    "drwxrwxrwx 1 eyamrog eyamrog       512 Apr 15 14:53  ./\n",
    "drwxrwxrwx 1 eyamrog eyamrog       512 Apr  1 03:07  ../\n",
    "drwxrwxrwx 1 eyamrog eyamrog       512 Apr 14 19:20  .ipynb_checkpoints/\n",
    "-rwxrwxrwx 1 eyamrog eyamrog       282 Mar 31 16:16  desktop.ini*\n",
    "-rwxrwxrwx 1 eyamrog eyamrog 231557811 Apr 15 14:49  text_uniq_first_view.tsv*\n",
    "-rwxrwxrwx 1 eyamrog eyamrog  58806097 Apr 15 14:53  text_uniq_first_view.xlsx*\n",
    "-rwxrwxrwx 1 eyamrog eyamrog 854050828 Apr 15 14:49  tweets_all2.tsv*\n",
    "-rwxrwxrwx 1 eyamrog eyamrog 109616884 Apr 15 14:52  tweets_all2.xlsx*\n",
    "-rwxrwxrwx 1 eyamrog eyamrog       165 Apr 15 14:53 '~$text_uniq_first_view.xlsx'*\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ grep -ic \"petista\" text_uniq_first_view.tsv\n",
    "3527\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ grep -ic \"lulista\" text_uniq_first_view.tsv\n",
    "383\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ grep -ic \"pistola\" text_uniq_first_view.tsv\n",
    "298\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ grep -ic \"entrevista\" text_uniq_first_view.tsv\n",
    "3812\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ grep -ic \"soldado\" text_uniq_first_view.tsv\n",
    "386\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ grep -ic \"comunista\" text_uniq_first_view.tsv\n",
    "2732\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ grep -ic \"cuba\" text_uniq_first_view.tsv\n",
    "973\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$\n",
    "\n",
    "20 a 50 mil tweets\n",
    "\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ grep -i \"pistola\" text_uniq_first_view.tsv > pistola.tsv\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ ll\n",
    "total 1224792\n",
    "drwxrwxrwx 1 eyamrog eyamrog       512 Apr 15 15:09  ./\n",
    "drwxrwxrwx 1 eyamrog eyamrog       512 Apr  1 03:07  ../\n",
    "drwxrwxrwx 1 eyamrog eyamrog       512 Apr 14 19:20  .ipynb_checkpoints/\n",
    "-rwxrwxrwx 1 eyamrog eyamrog         0 Apr 15 15:07  Scratch_Pad.txt*\n",
    "-rwxrwxrwx 1 eyamrog eyamrog       282 Mar 31 16:16  desktop.ini*\n",
    "-rwxrwxrwx 1 eyamrog eyamrog    148164 Apr 15 15:09  pistola.tsv*\n",
    "\n",
    "sample.tsv\n",
    "\n",
    "cat sample* > alltweets.tsv\n",
    "\n",
    "\n",
    "Para descobrir se 'Bolsonaro' estÃ¡ em todos os tweets que contÃ©m 'ladrÃ£o':\n",
    "grep -ic \"ladrÃ£o\" tweets_all2.tsv # Anote a quantidade\n",
    "grep -i \"ladrÃ£o\" tweets_all2.tsv > sample.tsv\n",
    "grep -ic \"Bolsonaro\" sample.tsv # Se a quantidade for a mesma, 'Bolsonaro' estÃ¡ em todos os tweets\n",
    "\n",
    "\n",
    "demoji package customisation\n",
    "\n",
    "change delimiters:\n",
    " \n",
    "open /opt/homebrew/lib/python3.10/site-packages/demoji/__init__.py\n",
    " \n",
    "then replace:\n",
    " \n",
    "def replace_with_desc(string, sep=\":\"):\n",
    " \n",
    "with:\n",
    " \n",
    "def replace_with_desc(string, sep=\"<\", sepb=\">\"):\n",
    " \n",
    "and replace:\n",
    " \n",
    " \n",
    "        result = result.replace(emoji, sep + desc + sep)\n",
    " \n",
    " \n",
    "with\n",
    " \n",
    "        result = result.replace(emoji, sep + desc + sepb)\n",
    " \n",
    "save file and run:\n",
    " \n",
    "demoji <text>\n",
    "has context menu\n",
    "\n",
    "RepositÃ³rio no Dropbox\n",
    "https://www.dropbox.com/scl/fo/7y8jjo3ev8wb7u6r8nvcr/AF5u0Lv39mc-fJv3RXLA8Fw?rlkey=6ogy1gxrtgka85dyqaftav1sc&dl=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34527c32",
   "metadata": {},
   "source": [
    "## Code snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910e3f92",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9b092",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Attach the IAM Role 'S3-Admin-Access' to the Ubuntu EC2 instance\n",
    "sudo apt update\n",
    "sudo apt upgrade -y\n",
    "# Reboot the EC2 instance from the AWS Console\n",
    "sudo apt install -y awscli\n",
    "# Set up Python virtual environment\n",
    "sudo apt install -y python3-pip\n",
    "sudo apt install -y python3-venv\n",
    "python3 -m venv my_env\n",
    "source \"$HOME\"/my_env/bin/activate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e6a7e2-0c55-4fb8-ab13-af6fff5cdaf0",
   "metadata": {},
   "source": [
    "### Setup and Execution in background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f1bfb-a1de-4cb3-9ce2-eac924f9a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up an EC2 instance (m5a.large)\n",
    "# Attach the 'S3-Admin-Access' IAM role to the EC2 instance\n",
    "git clone https://github.com/laelgelc/cl_st1_inrs.git\n",
    "cd cl_st1_inrs\n",
    "git config --global --edit\n",
    "bash setup_server.sh\n",
    "logout\n",
    "# Reboot the EC2 instance\n",
    "source ~/my_env/bin/activate\n",
    "cd cl_st1_inrs\n",
    "tar xzvf CL_St1_Ph2_INRS.tar.gz\n",
    "nohup python -u cl_st1_ph3_inrs.py &\n",
    "tail -f nohup.out\n",
    "nohup bash lmda.sh &\n",
    "deactivate\n",
    "logout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca26c2-0e44-46a9-b228-a8431032e96a",
   "metadata": {},
   "source": [
    "### Slicing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2341e0-fa51-4838-8c07-7b646efe0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting from row 8 onwards\n",
    "df = df.loc[8:]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5f737-fe6b-4394-849d-adb90b0ff5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting from the beginning until the row 504\n",
    "df = df.loc[:504]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18219ece-e45a-4fc7-ad1a-83a09d469bbc",
   "metadata": {},
   "source": [
    "### Setting input and output filenames using `argparse` and `RegEx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d186d-7645-47f2-92e3-85c271f71bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import argparse\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Setting the input and output filenames\n",
    "parser = argparse.ArgumentParser(description='Getting input filename.')\n",
    "parser.add_argument(\n",
    "    '--input_filename', help='Input filename.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "def add_pt_suffix(filename):\n",
    "    # Extract the base filename without the extension\n",
    "    base_filename = re.match(r'^([A-Za-z0-9-_,\\s]+)\\.[A-Za-z]{1,5}$', filename).group(1)\n",
    "    \n",
    "    # Append \"_pt\" to the base filename\n",
    "    new_filename = f'{base_filename}_pt'\n",
    "    \n",
    "    # Add the original file extension back\n",
    "    new_filename += re.search(r'\\.[A-Za-z]{1,5}$', filename).group()\n",
    "    \n",
    "    return new_filename\n",
    "\n",
    "input_filename = args.input_filename\n",
    "output_filename = add_pt_suffix(input_filename)\n",
    "\n",
    "print(input_filename)\n",
    "print(output_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba36aaef-48b5-43ec-be57-12a09efe5222",
   "metadata": {},
   "source": [
    "Test with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85069449-cd67-48a1-93ac-be2c9afde5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(my_env) eyamrog@Rog-ASUS:~$ python cl_st1_mariana_dataset.py --input_filename mari2016_1.jsonl\n",
    "mari2016_1.jsonl\n",
    "mari2016_1_pt.jsonl\n",
    "(my_env) eyamrog@Rog-ASUS:~$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b19ce6-9a84-491a-a6dd-3700e0ef3a3f",
   "metadata": {},
   "source": [
    "### Slicing the contents of a row with RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ffefc-feb2-4df9-b017-8bd407269ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[0, 'Speaker'] = 1 # Adding column 'Speaker' by initialising it with a numeric value in order to avoid DtypeWarning\n",
    "df = df.astype('object') # Converting the column to the desired data type\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    match = re.match(r'^([a-zA-Z0-9_., ]+):\\xa0', row['Text'])\n",
    "    if match:\n",
    "        speaker = match.group(1)\n",
    "        df.at[index, 'Speaker'] = speaker\n",
    "        text_without_speaker = row['Text'][len(match.group(0)):]\n",
    "        df.at[index, 'Text'] = text_without_speaker\n",
    "    else:\n",
    "        # Handle the case when no match is found (optional)\n",
    "        #df.at[index, 'Speaker'] = ''  # Set a default value\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff4d89f-3812-43c3-b210-8686521978ca",
   "metadata": {},
   "source": [
    "### Miscellaneous RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a2557-231a-4592-b7be-12c9cd24b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "moderators = re.match(r'(^\\w+)', moderators).group(1)\n",
    "\n",
    "participants = re.match(r'^\\w+ \\w+ (\\w+-\\w+)', participants).group(1)\n",
    "\n",
    "moderators = re.sub(r', \\w+$', '', moderators)\n",
    "\n",
    "df.loc[8, 'Text'] = re.sub(r'^\\[\\*\\] ', '', df.loc[8, 'Text'])\n",
    "\n",
    "df.loc[5, 'Text'] = re.sub(r'^\\w+ \\w+, \\w+\\[\\*\\]', '', df.loc[5, 'Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b011e-a430-4902-863c-85c411dc5517",
   "metadata": {},
   "source": [
    "### Checking and changing a dataframe cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "dfab94ea-ad81-4a37-be5c-55c1d9d889f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let me tell you: the media have been wrong before. We have never subsidized any country â or any company to move from the US to Latin America. You know full well the Caribbean Basin Initiative, youâve supported that.'"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[103, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "7a7779b2-15af-4bcc-b5a8-4a2c257656a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[103, 'Text'] = re.sub(r':', ' -', df.loc[103, 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "eaf0a508-614c-435a-8540-4a96ccceaff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let me tell you - the media have been wrong before. We have never subsidized any country â or any company to move from the US to Latin America. You know full well the Caribbean Basin Initiative, youâve supported that.'"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[103, 'Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3f621-5243-4f31-933e-9fff7ac7271d",
   "metadata": {},
   "source": [
    "### Dropping a row in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b661b6-f5b9-48dd-98e7-f2975421ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(477, inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.drop([1011, 1012, 1013], inplace=True)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aedf1e-97a5-44d5-a008-daf94040ecda",
   "metadata": {},
   "source": [
    "### Creating a column with the value of a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b22d12-495f-46a8-b607-48d1a256d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = df.at[0, 'Text']\n",
    "df['Title'] = title\n",
    "\n",
    "debate = df.at[2, 'Text']\n",
    "df['Debate'] = debate\n",
    "\n",
    "date = df.at[1, 'Text']\n",
    "df['Date'] = date\n",
    "\n",
    "participants = df.at[2, 'Text']\n",
    "participants = re.match(r'^\\w+ (\\w+-\\w+-\\w+)', participants).group(1)\n",
    "#participants = re.sub(r'^\\w+:\\n', '', participants)\n",
    "#participants = re.sub(r'\\n', ' ', participants)\n",
    "df['Participants'] = participants\n",
    "\n",
    "moderators = df.at[5, 'Text']\n",
    "moderators = re.match(r'(^\\w+ \\w+)', moderators).group(1)\n",
    "#moderators = re.sub(r'^\\w+:\\n', '', moderators)\n",
    "df['Moderators'] = moderators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14824e33",
   "metadata": {},
   "source": [
    "### Consolidating multiple JSONL files into one JSONL file and transferring them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e8ff5",
   "metadata": {},
   "source": [
    "#### Code snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c6bf58",
   "metadata": {},
   "source": [
    "##### Using `cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5c6ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mkdir mari\n",
    "\n",
    "cd mari\n",
    "\n",
    "aws s3 cp s3://gelcawsemr/2019_1/filtered_tweets.jsonl/ . --recursive\n",
    "\n",
    "find . -type f | wc -l\n",
    "\n",
    "cat *.json > mari2019_1.jsonl\n",
    "\n",
    "aws s3 cp mari2019_1.jsonl s3://laelgelcawsemrmariana/\n",
    "\n",
    "rm *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b344f97",
   "metadata": {},
   "source": [
    "##### Using `jq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37463b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sudo apt install -y jq\n",
    "\n",
    "mkdir mari\n",
    "\n",
    "cd mari\n",
    "\n",
    "aws s3 cp s3://gelcawsemr/2019_1/filtered_tweets.jsonl/ . --recursive\n",
    "\n",
    "find . -type f | wc -l\n",
    "\n",
    "find . -name '*.json' -print0 | xargs -0 jq -s '.' > mari2019_1.jsonl\n",
    "\n",
    "aws s3 cp mari2019_1.jsonl s3://laelgelcawsemrmariana/\n",
    "\n",
    "rm *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fd8fb",
   "metadata": {},
   "source": [
    "#### Via 'consolidate.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315ac08",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Set parameters\n",
    "path=\"2019_03\"\n",
    "filename=\"mari201903.jsonl\"\n",
    "origin_bucket=\"gelcawsemr\"\n",
    "destination_bucket=\"laelgelcawsemrmariana\"\n",
    "\n",
    "# Create input directory\n",
    "mkdir \"$HOME\"/\"$path\"/\n",
    "\n",
    "# Copy JSON files to the input directory\n",
    "aws s3 cp s3://\"$origin_bucket\"/\"$path\"/filtered_tweets.jsonl/ $HOME/\"$path\"/ --recursive\n",
    "\n",
    "# Consolidate the JSON files into a single \n",
    "cat \"$HOME\"/\"$path\"/*.json > $HOME/\"$path\"/\"$filename\"\n",
    "\n",
    "# Copy the consolidated file to the destination bucket\n",
    "aws s3 cp \"$HOME\"/\"$path\"/\"$filename\" s3://\"$destination_bucket\"/\n",
    "\n",
    "# Remove the input directory\n",
    "# rm -r \"$HOME\"/\"$path\"/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b393c6-84c9-4b1c-a9fd-01b347f25bbe",
   "metadata": {},
   "source": [
    "### Splitting large JSONL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c49643e-f320-4108-b742-588e39b35fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ ll\n",
    "total 2466300\n",
    "drwxrwxrwx 1 eyamrog eyamrog        512 Jun 19 13:04 ./\n",
    "drwxrwxrwx 1 eyamrog eyamrog        512 Jun  8 17:01 ../\n",
    "<omitted>\n",
    "-rwxrwxrwx 1 eyamrog eyamrog 2523737751 Jun 19 13:04 mari2017_1.jsonl*\n",
    "<omitted>\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sed -n '$=' mari2017_1.jsonl\n",
    "568559\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ echo '568559/2' | bc\n",
    "284279\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ split -l 284280 mari2017_1.jsonl\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sed -n '$=' xaa\n",
    "284280\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ ll\n",
    "total 4930924\n",
    "<omitted>\n",
    "-rwxrwxrwx 1 eyamrog eyamrog 2523737751 Jun 19 13:04 mari2017_1.jsonl*\n",
    "<omitted>\n",
    "-rwxrwxrwx 1 eyamrog eyamrog 1281293904 Jun 19 13:32 xaa*\n",
    "-rwxrwxrwx 1 eyamrog eyamrog 1242443847 Jun 19 13:33 xab*\n",
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403c113-2ae5-4160-a5b6-9f30774d94e8",
   "metadata": {},
   "source": [
    "### Listing Unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd81bf0-0f1e-4cc1-912a-b5c9fddf0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_unicode_characters(text):\n",
    "    # Define the RegEx pattern for matching Unicode characters\n",
    "    pattern = r'\\\\u[0-9A-Fa-f]{4}'\n",
    "    \n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    # Count the occurrences of each UNICODE character\n",
    "    char_count = Counter(matches)\n",
    "    \n",
    "    return char_count\n",
    "\n",
    "# Extract Unicode characters and their counts\n",
    "unicode_counts = extract_unicode_characters(df_true_json_prettified)\n",
    "\n",
    "# Print the results\n",
    "for char, count in unicode_counts.items():\n",
    "    print(f'Character {char}: Count = {count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d03f0d-1492-4763-93cc-9b953b3e9369",
   "metadata": {},
   "source": [
    "### Replacing Unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed7335-f64c-49f9-804c-869b6a134747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake_json_prettified_2 = df_fake_json_prettified.\\\n",
    "replace('\\\\u000b', '').\\\n",
    "replace('\\\\u0026', '&').\\\n",
    "replace('\\\\u003d', '=').\\\n",
    "replace('\\\\u00a0', '').\\\n",
    "replace('\\\\u00a1', 'Â¡').\\\n",
    "replace('\\\\u00a3', 'Â£').\\\n",
    "replace('\\\\u00ad', '').\\\n",
    "replace('\\\\u00af', 'Â¯').\\\n",
    "replace('\\\\u00b0', 'Â°').\\\n",
    "replace('\\\\u00b4', 'Â´').\\\n",
    "replace('\\\\u00bf', 'Â¿').\\\n",
    "replace('\\\\u00c9', 'Ã').\\\n",
    "replace('\\\\u00e0', 'Ã ').\\\n",
    "replace('\\\\u00e1', 'Ã¡').\\\n",
    "replace('\\\\u00e2', 'Ã¢').\\\n",
    "replace('\\\\u00e7', 'Ã§').\\\n",
    "replace('\\\\u00e9', 'Ã©').\\\n",
    "replace('\\\\u00ea', 'Ãª').\\\n",
    "replace('\\\\u00eb', 'Ã«').\\\n",
    "replace('\\\\u00ed', 'Ã­').\\\n",
    "replace('\\\\u00f1', 'Ã±').\\\n",
    "replace('\\\\u00f3', 'Ã³').\\\n",
    "replace('\\\\u00f4', 'Ã´').\\\n",
    "replace('\\\\u00f6', 'Ã¶').\\\n",
    "replace('\\\\u00fa', 'Ãº').\\\n",
    "replace('\\\\u00fc', 'Ã¼').\\\n",
    "replace('\\\\u0101', 'Ä').\\\n",
    "replace('\\\\u0107', 'Ä').\\\n",
    "replace('\\\\u014d', 'Å').\\\n",
    "replace('\\\\u017d', 'Å½').\\\n",
    "replace('\\\\u017e', 'Å¾').\\\n",
    "replace('\\\\u200a', ' ').\\\n",
    "replace('\\\\u200b', ' ').\\\n",
    "replace('\\\\u200B', ' ').\\\n",
    "replace('\\\\u200e', '').\\\n",
    "replace('\\\\u200f', '').\\\n",
    "replace('\\\\u2013', 'â').\\\n",
    "replace('\\\\u2014', 'â').\\\n",
    "replace('\\\\u2018', \"'\").\\\n",
    "replace('\\\\u2019', \"'\").\\\n",
    "replace('\\\\u201c', '\"').\\\n",
    "replace('\\\\u201d', '\"').\\\n",
    "replace('\\\\u2022', 'â¢').\\\n",
    "replace('\\\\u2026', 'â¦').\\\n",
    "replace('\\\\u2028', '(LS)').\\\n",
    "replace('\\\\u2029', '(PS)').\\\n",
    "replace('\\\\u202a', '(LRE)').\\\n",
    "replace('\\\\u202c', '').\\\n",
    "replace('\\\\u2032', \"'\").\\\n",
    "replace('\\\\u2033', \"''\").\\\n",
    "replace('\\\\u20ac', 'â¬').\\\n",
    "replace('\\\\u2122', '(TM)').\\\n",
    "replace('\\\\u2611', 'â').\\\n",
    "replace('\\\\u27a1', 'â¡').\\\n",
    "replace('\\\\u30c4', 'ã').\\\n",
    "replace('\\\\ufe0f', 'âï¸').\\\n",
    "replace('\\\\ufeff', '').\\\n",
    "replace('\\\\uffff', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3e5d8-363f-440e-9737-67b183b9ec73",
   "metadata": {},
   "source": [
    "### List of Identified Unicode characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ea345-cb6a-464d-b6ba-991738076543",
   "metadata": {},
   "source": [
    "[Unicode reference](https://www.compart.com/en/unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cf54e-dde3-488b-9c32-b3faab21d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\u000b = <Line Tabulation> (VT)\n",
    "\\u0026 = Ampersand = &\n",
    "\\u003d = Equals Sign = =\n",
    "\\u00a0 = No-Break Space (NBSP)\n",
    "\\u00a1 = Inverted Exclamation Mark = Â¡\n",
    "\\u00a3 = Pound Sign = Â£\n",
    "\\u00ad = Soft Hyphen (SHY)\n",
    "\\u00af = Macron = Â¯\n",
    "\\u00b0 = Degree Sign = Â°\n",
    "\\u00b4 = Acute Accent = Â´\n",
    "\\u00bf = Inverted Question Mark = Â¿\n",
    "\\u00c9 = Latin Capital Letter E with Acute = Ã\n",
    "\\u00e0 = Latin Small Letter A with Grave = Ã \n",
    "\\u00e1 = Latin Small Letter A with Acute = Ã¡\n",
    "\\u00e2 = Latin Small Letter A with Circumflex = Ã¢\n",
    "\\u00e7 = Latin Small Letter C with Cedilla = Ã§\n",
    "\\u00e9 = Latin Small Letter E with Acute = Ã©\n",
    "\\u00ea = Latin Small Letter E with Circumflex = Ãª\n",
    "\\u00eb = Latin Small Letter E with Diaeresis = Ã«\n",
    "\\u00ed = Latin Small Letter I with Acute = Ã­\n",
    "\\u00f1 = Latin Small Letter N with Tilde = Ã±\n",
    "\\u00f3 = Latin Small Letter O with Acute = Ã³\n",
    "\\u00f4 = Latin Small Letter O with Circumflex = Ã´\n",
    "\\u00f6 = Latin Small Letter O with Diaeresis = Ã¶\n",
    "\\u00fa = Latin Small Letter U with Acute = Ãº\n",
    "\\u00fc = Latin Small Letter U with Diaeresis = Ã¼\n",
    "\\u0101 = Latin Small Letter A with Macron = Ä\n",
    "\\u0107 = Latin Small Letter C with Acute = Ä\n",
    "\\u014d = Latin Small Letter O with Macron = Å\n",
    "\\u017d = Latin Capital Letter Z with Caron = Å½\n",
    "\\u017e = Latin Small Letter Z with Caron = Å¾\n",
    "\\u200a = Hair Space = ' '\n",
    "\\u200b = Zero Width Space (ZWSP) = ' '\n",
    "\\u200B = Zero Width Space (ZWSP) = ' '\n",
    "\\u200e = Left-to-Right Mark (LRM)\n",
    "\\u200f = Right-to-Left Mark (RLM)\n",
    "\\u2013 = En Dash = â\n",
    "\\u2014 = Em Dash = â\n",
    "\\u2018 = Left Single Quotation Mark = '\n",
    "\\u2019 = Right Single Quotation Mark = '\n",
    "\\u201c = Left Double Quotation Mark = \"\n",
    "\\u201d = Right Double Quotation Mark = \"\n",
    "\\u2022 = Bullet = â¢\n",
    "\\u2026 = Horizontal Ellipsis = â¦\n",
    "\\u2028 = Line Separator = (LS)\n",
    "\\u2029 = Paragraph Separator = (PS)\n",
    "\\u202a = Left-to-Right Embedding (LRE)\n",
    "\\u202c = Pop Directional Formatting (PDF)\n",
    "\\u2032 = Prime = '\n",
    "\\u2033 = Double Prime = ''\n",
    "\\u20ac = Euro Sign = â¬\n",
    "\\u2122 = Trade Mark Sign = (TM)\n",
    "\\u2611 = Ballot Box with Check = â\n",
    "\\u27a1 = Black Rightwards Arrow = â¡\n",
    "\\u30c4 = Katakana Letter Tu = ã\n",
    "\\ufe0f = Variation Selector-16 (VS16) = âï¸\n",
    "\\ufeff = Zero Width No-Break Space (BOM, ZWNBSP)\n",
    "\\uffff = Undefined Character\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0a26b-c77d-4056-8155-ffa163bef976",
   "metadata": {},
   "source": [
    "### Counting expressions in a certain dataframe column using RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21521c-c2f2-4462-a375-939b2d455f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emoji_counts(df, text_column):\n",
    "    \"\"\"\n",
    "    Extracts specifically formatted emoji counts from texts in a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the texts.\n",
    "        text_column (str): Name of the column containing text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with emoji counts.\n",
    "    \"\"\"\n",
    "    # Define the RegEx pattern for '{{Emoji:...}}'\n",
    "    emoji_pattern = r'EMOJI_.*?_e'\n",
    "\n",
    "    # Extract all matches from the specified text column\n",
    "    matches = df[text_column].str.findall(emoji_pattern)\n",
    "\n",
    "    # Flatten the list of matches\n",
    "    flat_matches = [match for sublist in matches for match in sublist]\n",
    "\n",
    "    # Count the occurrences of each emoji\n",
    "    emoji_counts = pd.Series(flat_matches).value_counts()\n",
    "\n",
    "    return emoji_counts\n",
    "\n",
    "# Get emoji counts\n",
    "emoji_counts_result = extract_emoji_counts(df_tweets_filtered, 'text')\n",
    "\n",
    "print('Emoji counts:')\n",
    "print(emoji_counts_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0c6ea-a0b1-4109-b8d7-fc3f36cb55d7",
   "metadata": {},
   "source": [
    "### Removing selected invisible Unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba43b716-3aef-42d4-924a-6318d00c773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sr presidente @jairbolsonaro, estÃ¡ na hora de desenvolvermos armas nucleares.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_invisible_unicode(text):\n",
    "    # Remove characters in the 'Format (Cf)' category (U+2066 and U+2069)\n",
    "    cleaned_text = re.sub(r'[\\u2066\\u2069]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage:\n",
    "text_unicode = \"Sr presidente â¦@jairbolsonaroâ©, estÃ¡ na hora de desenvolvermos armas nucleares.\"\n",
    "cleaned_text = remove_invisible_unicode(text_unicode)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e126dd3-862b-4c05-8be2-b7cd846384bc",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ff4c49-48c7-4418-ad70-c695fd815738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ! \" C3PO \" , are you there ? Go to http://google.com , or https://example.com and http://jaded.com.br https://t.co/U5wtRuWxSm , #case and ' find more ' about #comodore #bozo #daunting ! \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Defining a function to tokenise a string\n",
    "def tokenise_string(input_line):\n",
    "    # Replace URLs with placeholders\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\b'\n",
    "    placeholder = '<URL>'  # Choose a unique placeholder\n",
    "    urls = re.findall(url_pattern, input_line)\n",
    "    tokenised_line = re.sub(url_pattern, placeholder, input_line)  # Replace URLs with placeholders\n",
    "    \n",
    "    # Replace curly quotes with straight ones\n",
    "    tokenised_line = tokenised_line.replace('â', '\"').replace('â', '\"').replace(\"â\", \"'\").replace(\"â\", \"'\")\n",
    "    # Separate common punctuation marks with spaces\n",
    "    tokenised_line = re.sub(r'([.\\!?,\"\\'/()])', r' \\1 ', tokenised_line)\n",
    "    # Add a space before '#'\n",
    "    tokenised_line = re.sub(r'(?<!\\s)#', r' #', tokenised_line)  # Add a space before '#' if it is not already preceded by one\n",
    "    # Reduce extra spaces by a single space\n",
    "    tokenised_line = re.sub(r'\\s+', ' ', tokenised_line)\n",
    "    \n",
    "    # Replace the placeholders with the respective URLs\n",
    "    for url in urls:\n",
    "        tokenised_line = tokenised_line.replace(placeholder, url, 1)\n",
    "    \n",
    "    return tokenised_line\n",
    "\n",
    "line = 'Hello! âC3POâ,    are you   there?Go to http://google.com, or https://example.com and http://jaded.com.br  https://t.co/U5wtRuWxSm,    #case  and âfind moreâ about#comodore#bozo#daunting       !'\n",
    "tokenised_line = tokenise_string(line)\n",
    "\n",
    "print(tokenised_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf6bc8-6b7e-42d8-8c6b-69b8b4001c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
